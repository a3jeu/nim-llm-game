# üéÆ Confrontation des LLM au Jeu de Nim

![Screenshot du jeu](urlscreenshot.png)

## üìñ √Ä propos

Ce projet met en comp√©tition diff√©rents mod√®les de langage (LLM) dans le jeu classique de Nim. J'ai d√©velopp√© cette application pour me pratiquer √† int√©grer des LLM dans des applications interactives. C'est toujours plus motivant d'apprendre en s'amusant ! üöÄ

Le jeu supporte plusieurs variantes et permet de faire jouer des LLM entre eux, ou m√™me de jouer vous-m√™me contre un LLM.

**Connectons-nous !** üëâ [LinkedIn - Tommy Gagn√©](https://www.linkedin.com/in/tommygagne/)

## üéØ R√®gles du Jeu

Le jeu de Nim commence avec 21 b√¢tonnets. Chaque joueur retire √† tour de r√¥le un certain nombre de b√¢tonnets. Le joueur qui prend le dernier b√¢tonnet gagne la partie.

### Variantes disponibles

- **Normal** : Retirer 1 ou 2 b√¢tonnets
- **Variante A** : Si le nombre de b√¢tonnets est pair: 1, 2 ou 4 b√¢tonnets ; si impair: 1, 3 ou 4 b√¢tonnets
- **Variante B** : Retirer 1, 2 ou 3 b√¢tonnets. Il ne peut pas y avoir 2 tours cons√©cutifs o√π le m√™me nombre de b√¢tonnets est retir√©

## üöÄ Installation

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/a3jeu/nim-llm-game.git
cd nim-llm-game
```

### 2. Installer les d√©pendances

Ce projet utilise [uv](https://github.com/astral-sh/uv) pour la gestion des d√©pendances Python.

```bash
# Installer uv si ce n'est pas d√©j√† fait
pip install uv

# Installer les d√©pendances du projet
uv sync
```

### 3. Configurer les cl√©s API

Cr√©ez un fichier `.env` √† la racine du projet avec vos cl√©s API :

```env
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
DEEPSEEK_API_KEY=sk-...
GROQ_API_KEY=gsk_...
```

> **Note** : Vous n'avez pas besoin de toutes les cl√©s API. Le jeu fonctionnera avec les mod√®les dont vous avez configur√© les cl√©s.

### 4. Lancer l'application

```bash
uv run app.py
```

L'interface Gradio s'ouvrira automatiquement dans votre navigateur par d√©faut.

## ü¶ô Utiliser Ollama (Optionnel)

Ollama permet d'ex√©cuter des mod√®les LLM localement sur votre machine.

### Installation d'Ollama

1. T√©l√©chargez et installez Ollama depuis [ollama.ai](https://ollama.ai/)
2. T√©l√©chargez un mod√®le, par exemple :

```bash
ollama pull llama3.2
ollama pull qwen2.5
ollama pull mistral
```

### Configuration

Le jeu d√©tectera automatiquement les mod√®les Ollama disponibles si Ollama est en cours d'ex√©cution sur votre machine (port 11434 par d√©faut).

Aucune configuration suppl√©mentaire n'est n√©cessaire ! Les mod√®les Ollama appara√Ætront automatiquement dans la liste des mod√®les disponibles.

## üéÆ Utilisation

1. **S√©lectionner les joueurs** : Choisissez un mod√®le LLM pour chaque joueur (Rouge et Bleu), ou s√©lectionnez "Humain" pour jouer vous-m√™me
2. **Choisir la variante** : S√©lectionnez la variante du jeu que vous souhaitez jouer
3. **Lancer la partie** :
   - Cliquez sur "Lancer la partie" pour une partie automatique compl√®te
   - Cliquez sur "Prochain coup" pour avancer coup par coup
4. **Consulter les classements** : Allez dans l'onglet "Classement" pour voir les statistiques ELO

## üèÜ Syst√®me de Classement

Le jeu utilise un syst√®me de classement ELO pour √©valuer la performance des diff√©rents mod√®les :
- **ELO Global** : Performance sur toutes les variantes
- **ELO par variante** : Performance sp√©cifique √† chaque variante

## üõ†Ô∏è Technologies utilis√©es

- **Python 3.12+**
- **Gradio** : Interface utilisateur web
- **OpenAI API** : GPT-4, GPT-3.5
- **Anthropic API** : Claude 3.5, Claude 3
- **DeepSeek API** : DeepSeek Chat
- **Groq API** : Llama, Mixtral
- **Ollama** : Mod√®les locaux
- **SQLite** : Stockage des parties et classements

## üìù Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une pull request.

---

D√©velopp√© avec ‚ù§Ô∏è par [Tommy Gagn√©](https://www.linkedin.com/in/tommygagne/)
